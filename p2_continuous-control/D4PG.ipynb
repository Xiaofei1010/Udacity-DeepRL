{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><b><font size=10> CONTINUOUS CONTROL</font></b>\n",
    "#### <i>...implementation for Udacity Deep Reinforcement Learning \n",
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Imports for the notebook\n",
    "This Notebook uses code from separate python files where most of the implementation is handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import environment as E\n",
    "from buffers import ReplayBuffer\n",
    "from agent import D4PG_Agent\n",
    "import models\n",
    "from logger import Logger\n",
    "\n",
    "import os.path\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "import importlib\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "import torchvision.transforms as T\n",
    "# import multiprocessing as multi\n",
    "# multi.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually declare an ARGS class\n",
    "<i> For testing code in the notebook, to take the place of argparser in the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        self.nographics = False\n",
    "        self.num_eps = 10\n",
    "        self.rollout = 5\n",
    "        self.batchsize = 64\n",
    "        self.pretrain = 1000\n",
    "        self.num_episodes = 1\n",
    "        self.max_time = 50\n",
    "        self.alr = 1e-4\n",
    "        self.clr = 1e-4\n",
    "        self.batch = 128\n",
    "        self.buffer = 100000\n",
    "        self.C = 4000        \n",
    "        \n",
    "args = Args()\n",
    "def check_args():\n",
    "    for arg in vars(args):\n",
    "        if arg == \"sep\": continue\n",
    "        print(\"{}: {}\".format(arg.upper(), getattr(args, arg)))\n",
    "        \n",
    "        \n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Confirm that the args are all set the way we want them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: True\n",
      "NOGRAPHICS: False\n",
      "NUM_EPS: 10\n",
      "ROLLOUT: 5\n",
      "BATCHSIZE: 64\n",
      "PRETRAIN: 1000\n",
      "NUM_EPISODES: 1\n",
      "MAX_TIME: 50\n"
     ]
    }
   ],
   "source": [
    "check_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the environment\n",
    "<i> \n",
    "And print a bit of information contained in the wrapper class\n",
    "    \n",
    "Set the training mode to FALSE while interactively learning about the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 33\n",
      "Action size: 4\n",
      "Num Agents: 20\n"
     ]
    }
   ],
   "source": [
    "env = E.Environment(args, id=1)\n",
    "print(\"State size:\", env.state_size)\n",
    "print(\"Action size:\", env.action_size)\n",
    "print(\"Num Agents:\", env.agent_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Test code as it's developed\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take random actions in the environment below \n",
    "<i>\n",
    "-to check that code is working<br>\n",
    "-to get familiar with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "env.train = False\n",
    "env.reset()\n",
    "max_steps = 100\n",
    "scores = np.zeros(env.agent_count)\n",
    "arewards = np.empty((max_steps, env.agent_count))\n",
    "states = env.states\n",
    "actions = np.zeros((20,4))\n",
    "for i in range(max_steps):\n",
    "    actions = np.random.randn(env.agent_count, env.action_size)\n",
    "    #actions[:,3] += .01\n",
    "    actions = np.clip(actions, -1, 1)\n",
    "    next_states, rewards, dones = env.step(actions)\n",
    "    scores += rewards\n",
    "    arewards[i] = rewards\n",
    "    states = next_states\n",
    "#     if np.any(dones):\n",
    "#         break\n",
    "#    i += 1\n",
    "#print(arewards)\n",
    "print(arewards.min())\n",
    "print(arewards.max())\n",
    "#print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Force-reload modules as they're updated\n",
    "<i> This notebook was developed as the code is written in Atom, the below cell reloads the modules as they're needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import agent\n",
    "import buffers\n",
    "importlib.reload(models)\n",
    "importlib.reload(agent)\n",
    "importlib.reload(buffers)\n",
    "from agent import D4PG_Agent\n",
    "env.train = True\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the AGENT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing memory buffer.\n",
      "Taking pretrain step 10... memory filled: 120/1000                      \n",
      "Taking pretrain step 20... memory filled: 320/1000                      \n",
      "Taking pretrain step 30... memory filled: 520/1000                      \n",
      "Taking pretrain step 40... memory filled: 720/1000                      \n",
      "Taking pretrain step 50... memory filled: 920/1000                      \n",
      "Taking pretrain step 54... memory filled: 1000/1000                      \n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "d4pg_agent = D4PG_Agent(env.state_size, env.action_size, env.agent_count, args.alr, args.clr, args.batch, args.buffer, args.C)\n",
    "\n",
    "d4pg_agent.initialize_memory(args.pretrain, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.5254e-04,  4.7867e-04, -2.5832e-03,  ..., -9.0814e-04,\n",
      "          5.2277e-04, -6.8841e-04],\n",
      "        [-2.9081e-03,  5.7473e-04, -1.5296e-03,  ..., -2.5960e-03,\n",
      "          3.8295e-04, -4.7723e-04],\n",
      "        [ 2.3757e-03, -1.5391e-03,  1.0018e-03,  ..., -1.3681e-03,\n",
      "         -2.2576e-03,  2.4680e-07],\n",
      "        ...,\n",
      "        [ 1.3895e-03, -1.2050e-03,  2.0381e-03,  ..., -4.3795e-04,\n",
      "          2.8565e-03,  2.7703e-03],\n",
      "        [ 2.9882e-05,  1.6816e-03,  2.8541e-03,  ...,  1.1532e-03,\n",
      "          1.5181e-03, -9.6197e-04],\n",
      "        [-1.3362e-03, -1.0828e-03,  3.2171e-04,  ...,  8.2172e-04,\n",
      "         -1.3715e-03,  1.6301e-03]], device='cuda:0')\n",
      "tensor([[ 8.5254e-04,  4.7867e-04, -2.5832e-03,  ..., -9.0814e-04,\n",
      "          5.2277e-04, -6.8841e-04],\n",
      "        [-2.9081e-03,  5.7473e-04, -1.5296e-03,  ..., -2.5960e-03,\n",
      "          3.8295e-04, -4.7723e-04],\n",
      "        [ 2.3757e-03, -1.5391e-03,  1.0018e-03,  ..., -1.3681e-03,\n",
      "         -2.2576e-03,  2.4680e-07],\n",
      "        ...,\n",
      "        [ 1.3895e-03, -1.2050e-03,  2.0381e-03,  ..., -4.3795e-04,\n",
      "          2.8565e-03,  2.7703e-03],\n",
      "        [ 2.9882e-05,  1.6816e-03,  2.8541e-03,  ...,  1.1532e-03,\n",
      "          1.5181e-03, -9.6197e-04],\n",
      "        [-1.3362e-03, -1.0828e-03,  3.2171e-04,  ...,  8.2172e-04,\n",
      "         -1.3715e-03,  1.6301e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(d4pg_agent.critic.fc1.weight.data)\n",
    "print(d4pg_agent.critic_target.fc1.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out Actor actions without training\n",
    "<i> Test the <b>Actor</b> network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTIONS: [-0.12098932 -0.40055102 -0.23867819  0.08929716]\n",
      "ACTIONS: [-0.01125762  0.01922529  0.1803064  -0.13634835]\n",
      "ACTIONS: [ 0.6851838  -0.63353723  0.7436383   0.27492934]\n",
      "ACTIONS: [-0.4682395  -0.5609217  -0.74521905  0.36964744]\n",
      "ACTIONS: [-0.14885904 -0.20841734  0.5564802   0.27468184]\n",
      "ACTIONS: [-0.4888853   0.76849324 -0.1348718   0.6583199 ]\n",
      "ACTIONS: [ 0.33386275  0.06558588  0.05500592 -0.27297986]\n",
      "ACTIONS: [ 0.11389834  0.77768856 -0.10445781  0.23022468]\n",
      "ACTIONS: [-0.07206534  0.23954432 -0.37595558 -0.1366579 ]\n",
      "ACTIONS: [ 0.15717742 -0.03802539 -0.2943189   0.55542237]\n",
      "ACTIONS: [ 0.22255601 -0.28300273  0.03865694 -0.1659834 ]\n",
      "ACTIONS: [ 0.35342917  0.5857887  -0.08753023  0.5232274 ]\n",
      "ACTIONS: [-0.6818558   0.1905649   0.19634193 -0.2011529 ]\n",
      "ACTIONS: [0.05289635 0.20017947 0.36530805 0.08598615]\n",
      "ACTIONS: [ 0.02903109 -0.50989187  0.55134094 -0.02935818]\n",
      "ACTIONS: [-0.1647969  -0.45551774  0.7326171   0.21744782]\n",
      "ACTIONS: [-0.6101171   0.02357598 -0.1282186   0.16894275]\n",
      "ACTIONS: [-0.27840135 -0.06544321  0.26002723 -0.43684864]\n",
      "ACTIONS: [-0.13166472  0.20715335 -0.41868013 -0.3583507 ]\n",
      "ACTIONS: [-0.35893688 -0.18306711 -0.03606536 -0.20878433]\n",
      "ACTIONS: [ 0.3601365 -0.3139842  0.4120193 -0.1885678]\n",
      "ACTIONS: [ 0.33727798  0.2607298   0.14220034 -0.1221498 ]\n",
      "ACTIONS: [ 0.4536234  -0.06220143  0.0314963   0.57411236]\n",
      "ACTIONS: [-0.00680263  0.41416743 -0.33976138 -0.49490586]\n",
      "ACTIONS: [-0.02124485  0.2410533   0.2609735  -0.08844648]\n",
      "ACTIONS: [0.00803053 0.20077375 0.15102237 0.6645988 ]\n",
      "ACTIONS: [ 0.10059087 -0.01368898  0.74863625 -0.36815473]\n",
      "ACTIONS: [ 0.29351237 -0.24814062 -0.1307237  -0.01124   ]\n",
      "ACTIONS: [-0.5070097   0.14394253 -0.32050958 -0.02192372]\n",
      "ACTIONS: [-0.1473814  -0.0985198  -0.1287487  -0.11397912]\n",
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "scores = np.zeros(env.agent_count)\n",
    "states = env.states\n",
    "for i in range(30):\n",
    "    actions = d4pg_agent.act(states)\n",
    "    \n",
    "    # Print sample actions returned by the ACTOR network\n",
    "    print(\"ACTIONS:\", actions[1])\n",
    "    \n",
    "    next_states, rewards, dones = env.step(actions)\n",
    "    scores += rewards\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "    i += 1\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out Critic scores without training\n",
    "<i> Test the <b>Critic</b> network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIST:  tensor([[0.0149, 0.0166, 0.0122, 0.0199, 0.0156, 0.0281, 0.0148, 0.0267, 0.0185,\n",
      "         0.0232, 0.0132, 0.0190, 0.0155, 0.0180, 0.0229, 0.0159, 0.0125, 0.0248,\n",
      "         0.0230, 0.0221, 0.0251, 0.0157, 0.0151, 0.0314, 0.0177, 0.0172, 0.0197,\n",
      "         0.0187, 0.0175, 0.0179, 0.0171, 0.0179, 0.0194, 0.0250, 0.0129, 0.0216,\n",
      "         0.0217, 0.0263, 0.0148, 0.0218, 0.0229, 0.0179, 0.0200, 0.0259, 0.0173,\n",
      "         0.0204, 0.0284, 0.0173, 0.0253, 0.0185, 0.0143]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "Local DIST:  tensor([[0.0149, 0.0166, 0.0122, 0.0199, 0.0156, 0.0281, 0.0148, 0.0267, 0.0185,\n",
      "         0.0232, 0.0132, 0.0190, 0.0155, 0.0180, 0.0229, 0.0159, 0.0125, 0.0248,\n",
      "         0.0230, 0.0221, 0.0251, 0.0157, 0.0151, 0.0314, 0.0177, 0.0172, 0.0197,\n",
      "         0.0187, 0.0175, 0.0179, 0.0171, 0.0179, 0.0194, 0.0250, 0.0129, 0.0216,\n",
      "         0.0217, 0.0263, 0.0148, 0.0218, 0.0229, 0.0179, 0.0200, 0.0259, 0.0173,\n",
      "         0.0204, 0.0284, 0.0173, 0.0253, 0.0185, 0.0143]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "\n",
      "DIST:  tensor([[0.0208, 0.0153, 0.0143, 0.0226, 0.0157, 0.0227, 0.0143, 0.0264, 0.0231,\n",
      "         0.0233, 0.0153, 0.0262, 0.0187, 0.0168, 0.0263, 0.0130, 0.0124, 0.0168,\n",
      "         0.0233, 0.0256, 0.0214, 0.0135, 0.0180, 0.0282, 0.0166, 0.0160, 0.0208,\n",
      "         0.0228, 0.0129, 0.0206, 0.0185, 0.0186, 0.0154, 0.0235, 0.0180, 0.0200,\n",
      "         0.0171, 0.0200, 0.0121, 0.0256, 0.0237, 0.0225, 0.0173, 0.0244, 0.0248,\n",
      "         0.0200, 0.0261, 0.0159, 0.0192, 0.0205, 0.0133]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "Local DIST:  tensor([[0.0208, 0.0153, 0.0143, 0.0226, 0.0157, 0.0227, 0.0143, 0.0264, 0.0231,\n",
      "         0.0233, 0.0153, 0.0262, 0.0187, 0.0168, 0.0263, 0.0130, 0.0124, 0.0168,\n",
      "         0.0233, 0.0256, 0.0214, 0.0135, 0.0180, 0.0282, 0.0166, 0.0160, 0.0208,\n",
      "         0.0228, 0.0129, 0.0206, 0.0185, 0.0186, 0.0154, 0.0235, 0.0180, 0.0200,\n",
      "         0.0171, 0.0200, 0.0121, 0.0256, 0.0237, 0.0225, 0.0173, 0.0244, 0.0248,\n",
      "         0.0200, 0.0261, 0.0159, 0.0192, 0.0205, 0.0133]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "\n",
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "scores = np.zeros(env.agent_count)\n",
    "states = env.states\n",
    "for i in range(2):\n",
    "    actions = d4pg_agent.act(states) \n",
    "    ns, rewards, dones = env.step(actions)\n",
    "    scores += rewards\n",
    "    \n",
    "    # Print sample distributions returned by the CRITIC\n",
    "    batch = d4pg_agent.memory.sample(batch_size=1)\n",
    "    states = torch.cat(batch.state).to(DEVICE)\n",
    "    actions = torch.cat(batch.action).float().to(DEVICE)\n",
    "    rewards = torch.cat(batch.reward).to(DEVICE)\n",
    "    next_states = torch.cat(batch.next_state).to(DEVICE) \n",
    "\n",
    "    dist, probs = d4pg_agent.critic_target(next_states, d4pg_agent.actor(next_states))\n",
    "    proj_dist = d4pg_agent._get_targets(rewards, next_states)\n",
    "    ldist, lprobs = d4pg_agent.critic(next_states, d4pg_agent.actor(next_states))\n",
    "    \n",
    "    print(\"DIST: \", dist)\n",
    "    print(\"Local DIST: \", ldist)\n",
    "#     print(\"PROBS: \", probs)\n",
    "#     print(\"PROJECTED DIST: \", proj_dist)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    states = ns\n",
    "    if np.any(dones):\n",
    "        break\n",
    "    i += 1\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Test out various Agent functionality\n",
    "\n",
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.alr = 1e-4\n",
    "args.clr = 1e-4\n",
    "args.batch = 128\n",
    "args.buffer = 100000\n",
    "args.C = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import agent\n",
    "import buffers\n",
    "importlib.reload(models)\n",
    "importlib.reload(agent)\n",
    "importlib.reload(buffers)\n",
    "from agent import D4PG_Agent\n",
    "d4pg_agent = D4PG_Agent(env.state_size, env.action_size, env.agent_count, args.alr, args.clr, args.batch, args.buffer, args.C)\n",
    "env.train = True\n",
    "env.reset()\n",
    "d4pg_agent.initialize_memory(args.pretrain, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4pg_agent.initialize_memory(args.pretrain, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.max_time = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#w_old = torch.zeros(d4pg_agent.actor.output.weight.data[0].shape)\n",
    "for episode in range(1, 2):\n",
    "    # Begin each episode with a clean environment\n",
    "    env.reset()\n",
    "    # Get initial state\n",
    "    states = env.states\n",
    "    scores = np.zeros(env.agent_count)\n",
    "    # Gather experience for a maximum amount of steps, or until Done,\n",
    "    # whichever comes first\n",
    "    for t in range(args.max_time):\n",
    "        actions = d4pg_agent.act(states)\n",
    "        next_states, rewards, dones = env.step(actions)\n",
    "        d4pg_agent.step(states, actions, rewards, next_states)\n",
    "        states = next_states\n",
    "\n",
    "        scores += rewards\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    print(\"Episode rewards: \", scores.mean())\n",
    "    #print(scores)\n",
    "    #print(len(d4pg_agent.memory))\n",
    "    d4pg_agent.reset_nstep_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn about how the Categorical Bellman step works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.random.randn(env.agent_count, env.action_size)\n",
    "actions = np.clip(actions, -1, 1).astype(np.float32)\n",
    "states = torch.from_numpy(env.states).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, log_probs = critic(states, torch.from_numpy(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs.shape)\n",
    "print(log_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Container():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "c = Container()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = -10\n",
    "vmax = 10\n",
    "natoms = 51\n",
    "gamma = .99\n",
    "atoms = torch.linspace(vmin, vmax, natoms)\n",
    "delta_z = (vmax - vmin) / (natoms -1)\n",
    "r = torch.tensor(rewards).unsqueeze(-1)\n",
    "\n",
    "probs = probs.detach()\n",
    "q_next = (probs * atoms).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### projected atoms\n",
    "<html><i>\n",
    "<b>atoms.view(1,-1)</b> becomes shape [1, num_atoms]\n",
    "<br>\n",
    "<b>r</b> is unsqueezed in the last (-1) dimension, so it's shape [20,1]\n",
    "<br>\n",
    "the result is a tensor that holds an offset (projected) version of the atoms for each reward instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tz = projected atoms, atoms (values) projected by scaling and offsetting via the bellman equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = r + gamma * atoms.view(1,-1)\n",
    "tz.clamp_(vmin, vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### computes \"bj\" from the pseudo-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (tz - vmin) / c.delta_z\n",
    "b[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### l/u in the psuedocode are LOWER and UPPER bounds on the supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = b.floor().long()\n",
    "u = b.ceil().long()\n",
    "print(l[0])\n",
    "print(u[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### m_l/m_u are computed in the pseudocode under \"distribute the probability of tz\", but still a bit opaque to me on how it should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = (u.float() + (l == u).float() - b) * c.probs\n",
    "ml[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = (b - l.float()) * c.probs\n",
    "mu[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prob = torch.tensor(np.zeros(probs.size()))\n",
    "for i in range(target_prob.size(0)):\n",
    "    target_prob[i].index_add_(0, l[i].long(), ml[i].double())\n",
    "    target_prob[i].index_add_(0, u[i].long(), mu[i].double())\n",
    "target_prob[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the environment when finished with the code/training/etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
