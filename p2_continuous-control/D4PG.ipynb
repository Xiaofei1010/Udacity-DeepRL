{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><b><font size=10> CONTINUOUS CONTROL</font></b>\n",
    "#### <i>...implementation for Udacity Deep Reinforcement Learning \n",
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Imports for the notebook\n",
    "This Notebook uses code from separate python files where most of the implementation is handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import environment as E\n",
    "from buffers import ReplayBuffer\n",
    "from agent import D4PG_Agent\n",
    "import models\n",
    "from logger import Logger\n",
    "from progressbar import ProgressBar\n",
    "#from get_args import get_args\n",
    "\n",
    "import os.path\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "import importlib\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "import torchvision.transforms as T\n",
    "import multiprocessing as multi\n",
    "multi.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually declare an ARGS class\n",
    "<i> For testing code in the notebook, to take the place of argparser in the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        self.nographics = False\n",
    "        self.num_eps = 10\n",
    "        self.rollout = 5\n",
    "        self.batchsize = 64\n",
    "        self.pretrain = 1000\n",
    "        self.num_episodes = 1\n",
    "        self.max_time = 50\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Confirm that the args are all set the way we want them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: True\n",
      "NOGRAPHICS: False\n",
      "NUM_EPS: 10\n",
      "ROLLOUT: 5\n",
      "BATCHSIZE: 64\n",
      "PRETRAIN: 1000\n",
      "NUM_EPISODES: 1\n",
      "MAX_TIME: 50\n"
     ]
    }
   ],
   "source": [
    "for arg in vars(args):\n",
    "    if arg == \"sep\": continue\n",
    "    print(\"{}: {}\".format(arg.upper(), getattr(args, arg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the environment\n",
    "<i> \n",
    "And print a bit of information contained in the wrapper class\n",
    "    \n",
    "Set the training mode to FALSE while interactively learning about the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 33\n",
      "Action size: 4\n",
      "Num Agents: 20\n"
     ]
    }
   ],
   "source": [
    "env = E.Environment(args)\n",
    "print(\"State size:\", env.state_size)\n",
    "print(\"Action size:\", env.action_size)\n",
    "print(\"Num Agents:\", env.agent_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Test code as it's developed\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take random actions in the environment below \n",
    "<i>\n",
    "-to check that code is working<br>\n",
    "-to get familiar with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.train = False\n",
    "env.reset()\n",
    "scores = np.zeros(env.agent_count)\n",
    "states = env.states\n",
    "actions = np.zeros((20,4))\n",
    "for i in range(10):\n",
    "    actions = np.random.randn(env.agent_count, env.action_size)\n",
    "    #actions[:,3] += .01\n",
    "    actions = np.clip(actions, -1, 1)\n",
    "    next_states, rewards, dones = env.step(actions)\n",
    "    scores += rewards\n",
    "    states = next_states\n",
    "#     if np.any(dones):\n",
    "#         break\n",
    "#    i += 1\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Force-reload modules as they're updated\n",
    "<i> This notebook was developed as the code is written in Atom, the below cell reloads the modules as they're needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import agent\n",
    "importlib.reload(models)\n",
    "importlib.reload(agent)\n",
    "from agent import D4PG_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the AGENT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4pg_agent = D4PG_Agent(env.state_size, env.action_size, env.agent_count)\n",
    "# print(d4pg_agent.__class__.__name__)\n",
    "# print(d4pg_agent.memory)\n",
    "# agent.initialize_memory(10, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "states = env.states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out Actor actions without training\n",
    "<i> Test the <b>Actor</b> network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "scores = np.zeros(env.agent_count)\n",
    "states = env.states\n",
    "for i in range(30):\n",
    "    actions = d4pg_agent.act(states)\n",
    "    print(\"ACTIONS:\", actions[1])\n",
    "    next_states, rewards, dones = env.step(actions)\n",
    "    scores += rewards\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "    i += 1\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out Critic scores without training\n",
    "<i> Test the <b>Critic</b> network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "scores = np.zeros(env.agent_count)\n",
    "states = env.states\n",
    "for i in range(10):\n",
    "    actions = d4pg_agent.act(states) \n",
    "    next_states, rewards, dones = env.step(actions)\n",
    "    scores += rewards\n",
    "    dist, probs = d4pg_agent.critic(next_states, torch.from_numpy(actions))\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "    i += 1\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Test out various Agent functionality\n",
    "\n",
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pretrain = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing memory buffer.\n",
      "Taking pretrain step... 1, memory filled: 0/1000\n",
      "Taking pretrain step... 2, memory filled: 0/1000\n",
      "Taking pretrain step... 3, memory filled: 0/1000\n",
      "Taking pretrain step... 4, memory filled: 0/1000\n",
      "Taking pretrain step... 5, memory filled: 20/1000\n",
      "Taking pretrain step... 6, memory filled: 40/1000\n",
      "Taking pretrain step... 7, memory filled: 60/1000\n",
      "Taking pretrain step... 8, memory filled: 80/1000\n",
      "Taking pretrain step... 9, memory filled: 100/1000\n",
      "Taking pretrain step... 10, memory filled: 120/1000\n",
      "Taking pretrain step... 11, memory filled: 140/1000\n",
      "Taking pretrain step... 12, memory filled: 160/1000\n",
      "Taking pretrain step... 13, memory filled: 180/1000\n",
      "Taking pretrain step... 14, memory filled: 200/1000\n",
      "Taking pretrain step... 15, memory filled: 220/1000\n",
      "Taking pretrain step... 16, memory filled: 240/1000\n",
      "Taking pretrain step... 17, memory filled: 260/1000\n",
      "Taking pretrain step... 18, memory filled: 280/1000\n",
      "Taking pretrain step... 19, memory filled: 300/1000\n",
      "Taking pretrain step... 20, memory filled: 320/1000\n",
      "Taking pretrain step... 21, memory filled: 340/1000\n",
      "Taking pretrain step... 22, memory filled: 360/1000\n",
      "Taking pretrain step... 23, memory filled: 380/1000\n",
      "Taking pretrain step... 24, memory filled: 400/1000\n",
      "Taking pretrain step... 25, memory filled: 420/1000\n",
      "Taking pretrain step... 26, memory filled: 440/1000\n",
      "Taking pretrain step... 27, memory filled: 460/1000\n",
      "Taking pretrain step... 28, memory filled: 480/1000\n",
      "Taking pretrain step... 29, memory filled: 500/1000\n",
      "Taking pretrain step... 30, memory filled: 520/1000\n",
      "Taking pretrain step... 31, memory filled: 540/1000\n",
      "Taking pretrain step... 32, memory filled: 560/1000\n",
      "Taking pretrain step... 33, memory filled: 580/1000\n",
      "Taking pretrain step... 34, memory filled: 600/1000\n",
      "Taking pretrain step... 35, memory filled: 620/1000\n",
      "Taking pretrain step... 36, memory filled: 640/1000\n",
      "Taking pretrain step... 37, memory filled: 660/1000\n",
      "Taking pretrain step... 38, memory filled: 680/1000\n",
      "Taking pretrain step... 39, memory filled: 700/1000\n",
      "Taking pretrain step... 40, memory filled: 720/1000\n",
      "Taking pretrain step... 41, memory filled: 740/1000\n",
      "Taking pretrain step... 42, memory filled: 760/1000\n",
      "Taking pretrain step... 43, memory filled: 780/1000\n",
      "Taking pretrain step... 44, memory filled: 800/1000\n",
      "Taking pretrain step... 45, memory filled: 820/1000\n",
      "Taking pretrain step... 46, memory filled: 840/1000\n",
      "Taking pretrain step... 47, memory filled: 860/1000\n",
      "Taking pretrain step... 48, memory filled: 880/1000\n",
      "Taking pretrain step... 49, memory filled: 900/1000\n",
      "Taking pretrain step... 50, memory filled: 920/1000\n",
      "Taking pretrain step... 51, memory filled: 940/1000\n",
      "Taking pretrain step... 52, memory filled: 960/1000\n",
      "Taking pretrain step... 53, memory filled: 980/1000\n",
      "Taking pretrain step... 54, memory filled: 1000/1000\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "import agent\n",
    "import buffers\n",
    "importlib.reload(models)\n",
    "importlib.reload(agent)\n",
    "importlib.reload(buffers)\n",
    "from agent import D4PG_Agent\n",
    "d4pg_agent = D4PG_Agent(env.state_size, env.action_size, env.agent_count, )\n",
    "env.train = True\n",
    "env.reset()\n",
    "d4pg_agent.initialize_memory(args.pretrain, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4pg_agent.initialize_memory(args.pretrain, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.max_time = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#w_old = torch.zeros(d4pg_agent.actor.output.weight.data[0].shape)\n",
    "for episode in range(1, 2):\n",
    "    # Begin each episode with a clean environment\n",
    "    env.reset()\n",
    "    # Get initial state\n",
    "    states = env.states\n",
    "    scores = np.zeros(env.agent_count)\n",
    "    # Gather experience for a maximum amount of steps, or until Done,\n",
    "    # whichever comes first\n",
    "    for t in range(args.max_time):\n",
    "        actions = d4pg_agent.act(states)\n",
    "        next_states, rewards, dones = env.step(actions)\n",
    "        d4pg_agent.step(states, actions, rewards, next_states)\n",
    "        states = next_states\n",
    "\n",
    "        scores += rewards\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    print(\"Episode rewards: \", scores.mean())\n",
    "    #print(scores)\n",
    "    #print(len(d4pg_agent.memory))\n",
    "    d4pg_agent.reset_nstep_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4pg_agent.actor.fc1.weight.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn about how the Categorical Bellman step works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.random.randn(env.agent_count, env.action_size)\n",
    "actions = np.clip(actions, -1, 1).astype(np.float32)\n",
    "states = torch.from_numpy(env.states).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, probs, log_probs = critic(states, torch.from_numpy(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs.shape)\n",
    "print(log_probs.shape)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Container():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "c = Container()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.vmin = -10\n",
    "c.vmax = 10\n",
    "c.natoms = 51\n",
    "c.gamma = .99\n",
    "c.atoms = torch.linspace(vmin, vmax, natoms)\n",
    "c.delta_z = (vmax - vmin) / (natoms -1)\n",
    "c.r = torch.tensor(rewards).unsqueeze(-1)\n",
    "\n",
    "c.probs = probs.detach()\n",
    "c.q_next = (probs * atoms).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vars(x):\n",
    "    for arg in vars(x):\n",
    "        print(\"{}: {}\".format(arg.upper(), getattr(x, arg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### projected atoms\n",
    "<html><i>\n",
    "<b>atoms.view(1,-1)</b> becomes shape [1, num_atoms]\n",
    "<br>\n",
    "<b>r</b> is unsqueezed in the last (-1) dimension, so it's shape [20,1]\n",
    "<br>\n",
    "the result is a tensor that holds an offset (projected) version of the atoms for each reward instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tz = projected atoms, atoms (values) projected by scaling and offsetting via the bellman equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = c.r + c.gamma * c.atoms.view(1,-1)\n",
    "tz.clamp_(vmin, vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### computes \"bj\" from the pseudo-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (tz - c.vmin) / c.delta_z\n",
    "b[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### l/u in the psuedocode are LOWER and UPPER bounds on the supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = b.floor().long()\n",
    "u = b.ceil().long()\n",
    "print(l[0])\n",
    "print(u[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### m_l/m_u are computed in the pseudocode under \"distribute the probability of tz\", but still a bit opaque to me on how it should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version from GITHUB implementations\n",
    "\n",
    "gdml = (u.float() + (l == u).float() - b) * c.probs\n",
    "gdml[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version from GITHUB implementations\n",
    "\n",
    "gdmu = (b - l.float()) * c.probs\n",
    "gdmu[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version from the PAPER\n",
    "\n",
    "pdml = l.float() + (c.probs * (u.float()-b))\n",
    "pdml[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version from the PAPER\n",
    "\n",
    "pdmu = u.float() + (c.probs * (b - l.float()))\n",
    "pdmu[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prob.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.probs[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prob = torch.tensor(np.zeros(probs.size()))\n",
    "for i in range(target_prob.size(0)):\n",
    "    target_prob[i].index_add_(0, l[i].long(), gdml[i].double())\n",
    "    target_prob[i].index_add_(0, u[i].long(), gdmu[i].double())\n",
    "target_prob[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip dml and dmu together offset by 1 index and subtract 1, receive amount to add to probs!\n",
    "x = zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(c.probs.size(1)):\n",
    "    t = target_prob[0].numpy()[i]\n",
    "    p = c.probs[0].numpy()[i]\n",
    "    pad = 8\n",
    "    print(str(t)[:pad], str(p)[:pad], str(t-p)[:pad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time.struct_time(tm_year=2019, tm_mon=3, tm_mday=5, tm_hour=14, tm_min=32, tm_sec=43, tm_wday=1, tm_yday=64, tm_isdst=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.localtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1551817951.7659702"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.4375"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.localtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tm_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception calling application: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mattdoll\\Anaconda3\\envs\\drlnd\\lib\\multiprocessing\\connection.py\", line 312, in _recv_bytes\n",
      "    nread, err = ov.GetOverlappedResult(True)\n",
      "BrokenPipeError: [WinError 109] The pipe has been ended\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mattdoll\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\grpc\\_server.py\", line 385, in _call_behavior\n",
      "    return behavior(argument, context), True\n",
      "  File \"C:\\Users\\mattdoll\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\unityagents\\rpc_communicator.py\", line 26, in Exchange\n",
      "    return self.child_conn.recv()\n",
      "  File \"C:\\Users\\mattdoll\\Anaconda3\\envs\\drlnd\\lib\\multiprocessing\\connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"C:\\Users\\mattdoll\\Anaconda3\\envs\\drlnd\\lib\\multiprocessing\\connection.py\", line 321, in _recv_bytes\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
