{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><b><font size=10> CONTINUOUS CONTROL</font></b>\n",
    "#### <i>...implementation for Udacity Deep Reinforcement Learning \n",
    "<hr><hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Imports for the notebook\n",
    "This Notebook uses code from separate python files where most of the implementation is handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import environment as E\n",
    "from buffers import ReplayBuffer, nStepBuffer\n",
    "from agent import D4PG_Agent\n",
    "\n",
    "#from get_args import get_args\n",
    "\n",
    "import os.path\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "import importlib\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "import torchvision.transforms as T\n",
    "import multiprocessing as multi\n",
    "multi.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually declare an ARGS class\n",
    "<i> For testing code in the notebook, to take the place of argparser in the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        self.nographics = False\n",
    "        self.num_eps = 10\n",
    "        self.rollout = 5\n",
    "        self.batchsize = 64\n",
    "        self.pretrain = 1000\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Confirm that the args are all set the way we want them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: True\n",
      "NOGRAPHICS: False\n",
      "NUM_EPS: 10\n",
      "ROLLOUT: 5\n",
      "BATCHSIZE: 64\n",
      "PRETRAIN: 1000\n"
     ]
    }
   ],
   "source": [
    "for arg in vars(args):\n",
    "    if arg == \"sep\": continue\n",
    "    print(\"{}: {}\".format(arg.upper(), getattr(args, arg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the environment\n",
    "<i> & print a bit of information contained in the wrapper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 33\n",
      "Action size: 4\n",
      "Num Agents: 20\n"
     ]
    }
   ],
   "source": [
    "env = E.Environment(args)\n",
    "print(\"State size:\", env.state_size)\n",
    "print(\"Action size:\", env.action_size)\n",
    "print(\"Num Agents:\", env.agent_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Test code as it's developed\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take random actions in the environment below \n",
    "<i>\n",
    "-to check that code is working<br>\n",
    "-to get familiar with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.train = False\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "scores = np.zeros(env.agent_count)\n",
    "states = env.states\n",
    "for i in range(20):\n",
    "    actions = np.random.randn(env.agent_count, env.action_size)\n",
    "    actions = np.clip(actions, -1, 1)\n",
    "    next_states, rewards, dones = env.step(actions)\n",
    "    scores += rewards\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "    i += 1\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Force-reload modules as they're updated\n",
    "<i> This notebook was developed as the code is written in Atom, the below cell reloads the modules as they're needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agent\n",
    "importlib.reload(agent)\n",
    "#importlib.reload(E)\n",
    "importlib.reload(models)\n",
    "from agent import D4PG_Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4pg_agent = D4PG_Agent(env.state_size, env.action_size, env.agent_count)\n",
    "# print(d4pg_agent.__class__.__name__)\n",
    "# print(d4pg_agent.memory)\n",
    "# agent.initialize_memory(10, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "states = env.states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out Actor actions without training\n",
    "<i> Test the <b>Actor</b> network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTIONS: [ 0.39250484  0.10228997  0.31567538 -0.41675773]\n",
      "ACTIONS: [ 0.22899312  0.2972572   0.08953724 -0.07314225]\n",
      "ACTIONS: [ 0.5929218  -0.0166251  -0.22665542 -0.22307695]\n",
      "ACTIONS: [ 0.4252136  -0.04706254 -0.29255268  0.5514186 ]\n",
      "ACTIONS: [ 0.00659929  0.16880164 -0.15172873  0.34427068]\n",
      "ACTIONS: [ 0.4874601  -0.05414694  0.18998487 -0.31815657]\n",
      "ACTIONS: [0.5247522  0.28847387 0.7349592  0.18429683]\n",
      "ACTIONS: [0.45776945 0.17300053 0.3757362  0.21077505]\n",
      "ACTIONS: [ 0.54964024 -0.19631551  0.07580662  0.31457314]\n",
      "ACTIONS: [ 0.32845622 -0.19134298 -0.48707473  0.02579423]\n",
      "ACTIONS: [-0.03437502 -0.01179501  0.08437829  0.3584921 ]\n",
      "ACTIONS: [-0.06189606 -0.21877146  0.16300105 -0.22402516]\n",
      "ACTIONS: [ 0.06274641  0.61312693 -0.26764357  0.17627218]\n",
      "ACTIONS: [-0.21768238  0.0877857  -0.01208796  0.05560011]\n",
      "ACTIONS: [-0.4288527   0.2777754  -0.31315735 -0.13877532]\n",
      "ACTIONS: [-0.64553875  0.30576026 -0.10319347 -0.07020348]\n",
      "ACTIONS: [-0.22993162 -0.1994104  -0.1120761   0.17905626]\n",
      "ACTIONS: [-0.04623746 -0.5734256  -0.24837199 -0.11657237]\n",
      "ACTIONS: [ 0.12893169 -0.55361253  0.21750483  0.25660816]\n",
      "ACTIONS: [-0.12219419 -0.42916808  0.4450274  -0.27714843]\n",
      "ACTIONS: [-0.02641164 -0.10870197 -0.28319284 -0.22797285]\n",
      "ACTIONS: [-0.64588356  0.0443129   0.10603403 -0.44444686]\n",
      "ACTIONS: [-0.6207973  -0.00690367  0.47024092 -0.19487795]\n",
      "ACTIONS: [ 0.56228995  0.11573084 -0.10544166  0.06556372]\n",
      "ACTIONS: [ 0.39813802 -0.29807156 -0.3161325   0.08077937]\n",
      "ACTIONS: [-0.23780064 -0.15488812 -0.24278171 -0.15826377]\n",
      "ACTIONS: [ 0.12635717 -0.33563942 -0.1461776   0.07579098]\n",
      "ACTIONS: [-0.2842203   0.26517716  0.06995561  0.37768632]\n",
      "ACTIONS: [-0.02349428  0.07753317  0.12802826  0.17067091]\n",
      "ACTIONS: [ 0.03425889  0.44557834  0.17985982 -0.05610539]\n",
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "scores = np.zeros(env.agent_count)\n",
    "states = env.states\n",
    "for i in range(30):\n",
    "    #actions = agent.actor(torch.from_numpy(states).float()).detach().numpy()\n",
    "    #actions += agent.gauss_noise(actions.shape)\n",
    "    #actions = np.clip(actions, -1, 1)\n",
    "    actions = d4pg_agent.act(states)\n",
    "    print(\"ACTIONS:\", actions[1])\n",
    "    next_states, rewards, dones = env.step(actions)\n",
    "    scores += rewards\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "    i += 1\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out Critic scores without training\n",
    "<i> Test the <b>Critic</b> network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "scores = np.zeros(env.agent_count)\n",
    "states = env.states\n",
    "for i in range(10):\n",
    "    actions = d4pg_agent.act(states) \n",
    "    next_states, rewards, dones = env.step(actions)\n",
    "    scores += rewards\n",
    "    q, probs = d4pg_agent.critic(next_states, torch.from_numpy(actions))\n",
    "    #print(q.shape, q)\n",
    "    #print(values.sample())\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "    i += 1\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.random.randn(env.agent_count, env.action_size)\n",
    "actions = np.clip(actions, -1, 1).astype(np.float32)\n",
    "states = torch.from_numpy(env.states).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, probs, log_probs = critic(states, torch.from_numpy(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs.shape)\n",
    "print(log_probs.shape)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.tensor(rewards).unsqueeze(-1)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.0000,  -9.6000,  -9.2000,  -8.8000,  -8.4000,  -8.0000,  -7.6000,\n",
       "         -7.2000,  -6.8000,  -6.4000,  -6.0000,  -5.6000,  -5.2000,  -4.8000,\n",
       "         -4.4000,  -4.0000,  -3.6000,  -3.2000,  -2.8000,  -2.4000,  -2.0000,\n",
       "         -1.6000,  -1.2000,  -0.8000,  -0.4000,   0.0000,   0.4000,   0.8000,\n",
       "          1.2000,   1.6000,   2.0000,   2.4000,   2.8000,   3.2000,   3.6000,\n",
       "          4.0000,   4.4000,   4.8000,   5.2000,   5.6000,   6.0000,   6.4000,\n",
       "          6.8000,   7.2000,   7.6000,   8.0000,   8.4000,   8.8000,   9.2000,\n",
       "          9.6000,  10.0000])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmin = -10\n",
    "vmax = 10\n",
    "natoms = 51\n",
    "gamma = .99\n",
    "atoms = torch.linspace(vmin, vmax, natoms)\n",
    "atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.0000,  -9.6000,  -9.2000,  -8.8000,  -8.4000,  -8.0000,  -7.6000,\n",
       "          -7.2000,  -6.8000,  -6.4000,  -6.0000,  -5.6000,  -5.2000,  -4.8000,\n",
       "          -4.4000,  -4.0000,  -3.6000,  -3.2000,  -2.8000,  -2.4000,  -2.0000,\n",
       "          -1.6000,  -1.2000,  -0.8000,  -0.4000,   0.0000,   0.4000,   0.8000,\n",
       "           1.2000,   1.6000,   2.0000,   2.4000,   2.8000,   3.2000,   3.6000,\n",
       "           4.0000,   4.4000,   4.8000,   5.2000,   5.6000,   6.0000,   6.4000,\n",
       "           6.8000,   7.2000,   7.6000,   8.0000,   8.4000,   8.8000,   9.2000,\n",
       "           9.6000,  10.0000]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atoms.view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atoms_delta = (vmax - vmin) / (natoms -1)\n",
    "atoms_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0196, 0.0197, 0.0196,  ..., 0.0196, 0.0196, 0.0196],\n",
       "        [0.0196, 0.0197, 0.0196,  ..., 0.0196, 0.0196, 0.0196],\n",
       "        [0.0196, 0.0197, 0.0196,  ..., 0.0196, 0.0196, 0.0196],\n",
       "        ...,\n",
       "        [0.0196, 0.0197, 0.0196,  ..., 0.0196, 0.0196, 0.0196],\n",
       "        [0.0196, 0.0197, 0.0196,  ..., 0.0196, 0.0196, 0.0196],\n",
       "        [0.0196, 0.0197, 0.0196,  ..., 0.0196, 0.0196, 0.0196]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = probs.detach()\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0289)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_next = (probs * atoms).sum()\n",
    "q_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(r.shape)\n",
    "print(atoms.view(1,-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "atoms.view(1,-1) becomes shape [1, num_atoms]\n",
    "\n",
    "r is unsqueezed in the last (-1) dimension, so it's shape [20,1]\n",
    "\n",
    "the result is a tensor that holds an offset (projected) version of the atoms for each reward instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.9000, -9.5040, -9.1080,  ...,  9.1080,  9.5040,  9.9000],\n",
       "        [-9.9000, -9.5040, -9.1080,  ...,  9.1080,  9.5040,  9.9000],\n",
       "        [-9.9000, -9.5040, -9.1080,  ...,  9.1080,  9.5040,  9.9000],\n",
       "        ...,\n",
       "        [-9.9000, -9.5040, -9.1080,  ...,  9.1080,  9.5040,  9.9000],\n",
       "        [-9.9000, -9.5040, -9.1080,  ...,  9.1080,  9.5040,  9.9000],\n",
       "        [-9.9000, -9.5040, -9.1080,  ...,  9.1080,  9.5040,  9.9000]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projected_atoms = r + gamma * atoms.view(1,-1)\n",
    "projected_atoms.clamp_(vmin, vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2500,  1.2400,  2.2300,  ..., 47.7700, 48.7600, 49.7500],\n",
       "        [ 0.2500,  1.2400,  2.2300,  ..., 47.7700, 48.7600, 49.7500],\n",
       "        [ 0.2500,  1.2400,  2.2300,  ..., 47.7700, 48.7600, 49.7500],\n",
       "        ...,\n",
       "        [ 0.2500,  1.2400,  2.2300,  ..., 47.7700, 48.7600, 49.7500],\n",
       "        [ 0.2500,  1.2400,  2.2300,  ..., 47.7700, 48.7600, 49.7500],\n",
       "        [ 0.2500,  1.2400,  2.2300,  ..., 47.7700, 48.7600, 49.7500]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = (projected_atoms - vmin) / atoms_delta\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
      "        35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n",
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "        19, 20, 21, 22, 23, 24, 25, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])\n"
     ]
    }
   ],
   "source": [
    "l = b.floor().long()\n",
    "u = b.ceil().long()\n",
    "print(l[1])\n",
    "print(u[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0147, 0.0149, 0.0151, 0.0153, 0.0155, 0.0157, 0.0159, 0.0160, 0.0163,\n",
       "        0.0165, 0.0167, 0.0169, 0.0171, 0.0173, 0.0174, 0.0177, 0.0178, 0.0180,\n",
       "        0.0182, 0.0184, 0.0187, 0.0188, 0.0190, 0.0192, 0.0194, 0.0196, 0.0002,\n",
       "        0.0004, 0.0006, 0.0008, 0.0010, 0.0012, 0.0014, 0.0016, 0.0018, 0.0020,\n",
       "        0.0022, 0.0024, 0.0026, 0.0027, 0.0029, 0.0031, 0.0033, 0.0035, 0.0037,\n",
       "        0.0039, 0.0041, 0.0043, 0.0045, 0.0047, 0.0049])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dml = (u + (l == u).float() - b) * probs\n",
    "dml[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4678e-02, 1.0149e+00, 2.0151e+00,  ..., 4.7005e+01, 4.8005e+01,\n",
       "         4.9005e+01],\n",
       "        [1.4680e-02, 1.0149e+00, 2.0151e+00,  ..., 4.7005e+01, 4.8005e+01,\n",
       "         4.9005e+01],\n",
       "        [1.4678e-02, 1.0149e+00, 2.0151e+00,  ..., 4.7005e+01, 4.8005e+01,\n",
       "         4.9005e+01],\n",
       "        ...,\n",
       "        [1.4678e-02, 1.0149e+00, 2.0151e+00,  ..., 4.7005e+01, 4.8005e+01,\n",
       "         4.9005e+01],\n",
       "        [1.4679e-02, 1.0149e+00, 2.0151e+00,  ..., 4.7005e+01, 4.8005e+01,\n",
       "         4.9005e+01],\n",
       "        [1.4679e-02, 1.0149e+00, 2.0151e+00,  ..., 4.7005e+01, 4.8005e+01,\n",
       "         4.9005e+01]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dml = l.float() + (probs * (u.float()-b))\n",
    "dml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0049, 0.0047, 0.0045, 0.0043, 0.0041, 0.0039, 0.0037, 0.0035, 0.0033,\n",
       "        0.0031, 0.0029, 0.0027, 0.0025, 0.0024, 0.0022, 0.0020, 0.0018, 0.0016,\n",
       "        0.0014, 0.0012, 0.0010, 0.0008, 0.0006, 0.0004, 0.0002, 0.0000, 0.0194,\n",
       "        0.0193, 0.0190, 0.0188, 0.0186, 0.0184, 0.0182, 0.0180, 0.0179, 0.0177,\n",
       "        0.0174, 0.0173, 0.0171, 0.0168, 0.0166, 0.0164, 0.0163, 0.0160, 0.0159,\n",
       "        0.0157, 0.0155, 0.0153, 0.0151, 0.0149, 0.0147])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmu = (b - l.float()) * probs\n",
    "dmu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 51])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_prob = torch.tensor(np.zeros(probs.size()))\n",
    "target_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.8712e-02, 1.0744e+00, 2.0746e+00, 3.0748e+00, 4.0748e+00, 5.0752e+00,\n",
       "        6.0754e+00, 7.0753e+00, 8.0757e+00, 9.0760e+00, 1.0076e+01, 1.1076e+01,\n",
       "        1.2076e+01, 1.3077e+01, 1.4077e+01, 1.5077e+01, 1.6077e+01, 1.7077e+01,\n",
       "        1.8078e+01, 1.9078e+01, 2.0078e+01, 2.1078e+01, 2.2079e+01, 2.3079e+01,\n",
       "        2.4079e+01, 5.0060e+01, 2.6060e+01, 2.7060e+01, 2.8060e+01, 2.9060e+01,\n",
       "        3.0061e+01, 3.1061e+01, 3.2061e+01, 3.3061e+01, 3.4062e+01, 3.5062e+01,\n",
       "        3.6062e+01, 3.7062e+01, 3.8062e+01, 3.9062e+01, 4.0062e+01, 4.1063e+01,\n",
       "        4.2063e+01, 4.3063e+01, 4.4063e+01, 4.5064e+01, 4.6064e+01, 4.7064e+01,\n",
       "        4.8064e+01, 4.9064e+01, 4.4159e-02], dtype=torch.float64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(target_prob.size(0)):\n",
    "    target_prob[i].index_add_(0, l[i].long(), dml[i].double())\n",
    "    target_prob[i].index_add_(0, u[i].long(), dmu[i].double())\n",
    "target_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
