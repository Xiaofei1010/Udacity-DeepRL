{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CONTINUOUS CONTROL** - Reacher\n",
    "\n",
    "### *Implementation for Udacity Deep Reinforcement Learning*\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load a few extensions before doing anything else..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview, Results, & To-Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent overview\n",
    "\n",
    "#### This codebase implements a [**Distributed Distributional Deep Deterministic Policy Gradient (D4PG)**](https://arxiv.org/pdf/1804.08617.pdf) agent based upon the groundbreaking work by Google DeepMind.\n",
    "\n",
    "\n",
    "D4PG is itself an evolution out of the paper [_\"Continuous Control with Deep Reinforcement Learning (DDPG)\"_](https://arxiv.org/pdf/1509.02971.pdf) also by Google DeepMind.\n",
    "\n",
    "\n",
    "DDPG is an architecture which allows for an **Actor** network to estimate a deterministic action policy while determining the _value_ of that action using a **Critic** network that outputs a Q-value by using the Action as part of the input.\n",
    "\n",
    "_The paper introducing D4PG introduces several key improvements over the algorithm described in the original DDPG paper:_\n",
    "* K-Actor distributed training\n",
    "* Prioritized Replay buffer\n",
    "* N-Step Rollout/Bootstrapping\n",
    "* **Distributional Value Estimation**\n",
    "\n",
    "In my opinion, the most important addition here is the _Distributional_ step. The other steps have been studied and/or implemented in other papers or textbooks, and feel more evolutionary to the DDPG algorithm than a truly new algorithm.\n",
    "\n",
    "_**Distributional Value Estimation**_ allows the agent to not just estimate the value of a given action, but to estimate the probability of a value, thus allowing for uncertainty in the return. I certainly feel, and agree with the research that posits the idea, that allowing an agent to say \"This is X% likely to result in this value, but also Y% in this other value\" is much more like how a human, or intelligent creature, learns. At the very least it adds robustness to training by allowing for further complexity in comparing online and offline network estimations and thus improves loss calculations.\n",
    "\n",
    "This portion of the research is credited to:  \n",
    "[_\"A Distributional Perspective on Reinforcement Learning\"_](https://arxiv.org/pdf/1707.06887.pdf)  \n",
    "And should be considered a must-read for anyone interested in the concept.\n",
    "\n",
    "The D4PG paper suggested that **Prioritized Experience Replay (PER)** did not result in demonstrable improvements in training, and thus, it was not implemented here. The results achieved by this agent do not seem to suffer for it's lack, but it would be good to test with and without to be thorough in evaluating training performance.\n",
    "\n",
    "This agent does use **N-Step Bootstrapping** as described in the paper. N-Step bootstrapping performs Temporal-Difference (TD) Learning across N steps instead of a single SARS' tuple. In a plentiful-reward environment such as _Reacher_, removing uncertainty from value estimation is a huge win, and while I did not run trials of differing values for `N`, it is likely that larger values would have some positive effect, similarly to a PPO agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because who doesn't like to see the results before the details...\n",
    "\n",
    "In the Reacher environment, the D4PG algorithm is robust to a very wide variety of hyperparameters, below are a few examples.\n",
    "\n",
    "In all "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![D4PG_v1_graph.png](images/D4PG_v1_graph.png)\n",
    "![D4PG_v2_graph.png](images/D4PG_v2_graph.png)\n",
    "![D4PG_v3_graph.png](images/D4PG_v3_graph.png)\n",
    "![D4PG_v4_graph.png](images/D4PG_v4_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters\n",
    "\n",
    "**DDQN** has proven robust to a fairly wide variety of hyperparameters. \n",
    "* **`LEARN_RATE`** The above graphs show an order of magnitude variance in Learning Rate while still converging to optimal performance.\n",
    "* **`C/TAU`** I have consistently, across all projects, found that a HARD update type utilizing a parameter C controlling how many timesteps between updates, gives me superior performance over soft-update style gradual target network updating using parameter TAU. In the above graphs, C is consistently set to 650, though 720 is used in  the third. The relationship between training and updates seems to remain somewhat ambiguous, however, for this project, ensuring that some small multiple of episodes passes before updating the network provided good results. As this Banana environment performed 300 timesteps before reaching a Done state, 650-750 was a good range of allowing for learning before updating the networks.\n",
    "* **`EPSILON`** Annealing Epsilon at a slow rate was key to stable and fast learning as well, allowing a balance between exploration and exploitation. In this project, Epsilon annealing is has no relationship with actual Agent performance, and in later code this is no longer the case. Epsilon annealing rate should have some relationship with Agent performance for the most consistent results without requiring hand tuning. However, this requires some knowledge of the environment about *what constitutes a good score or good performance.*\n",
    "* **`PRETRAIN`** Interestingly, it was found that filling the ReplayBuffer with many more memories than required to sample a batch led to significantly worse performance or non-convergence. This is counterinuitive and in fact the opposite of findings from other projects, and should be further studied. Regardless, for Banana, pretrain was best left at a low number.\n",
    "* **`ROLLOUT`** As stated above, n-step rollout was allowed to run in this environment, but was inconsequential to the results. This parameter could be set to 1 and achieve the same scores. Theoretically, having a rollout contribution should stabilize learning, but there was not computing bandwidth to run trials of this.\n",
    "* **`L2_DECAY`** Adding a very small Adam Optimizer l2 decay seemed to be a benefit, this parameter was never tuned in a meaningful way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Ideas\n",
    "\n",
    "* This was my first Reinforcement Learning project and as such some of the code is not as pretty as it could/should be, or as flexible, and I would like to circle back to implement DQN/DDQN into the more mature code structure of the later projects\n",
    "* **Prioritized Experience Replay (PER)** (https://arxiv.org/abs/1511.05952) was implemented, but never fully tested. I would like to optimize and adapt this more generally for future agents of all frameworks. Although this class did not require PER to achieve fast, excellent results on any projects, this would be a great tool to add into any Reinforcement Learning framework.\n",
    "* **Dueling DQN** would be good to explore further. While I became familiar with the concepts of an Advantage function (which is the key contribution of Dueling DQN to the overall algorithm) during study, I have yet to implement an agent with an Advantage function portion. This is key to many policy based algorithms and is a high priority for further study.\n",
    "* A so-called **\"Rainbow\"** implementation would utilize all of these tools to produce a superior agent.\n",
    "* **Visual/Pixel based training** was implemented but was not successful, in this project, at converging in any predictable way. Due to time constraints I moved on to other projects and more advanced/state-of-the-art algorithms, but training from visual data is extremely important to machine learning in general and I need to devote further time to a strong implementation of convolutional neural network based training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Click below to watch the trained Agent!\n",
    "[![\"Train Agent\"](http://img.youtube.com/vi/4bjDPNpwya0/0.jpg)](https://www.youtube.com/watch?v=4bjDPNpwya0 \"Banana Collector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Imports\n",
    "This Notebook uses code from separate python files where most of the implementation is handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from agent import DQN_Agent\n",
    "from environment import Environment\n",
    "from data_handling import Logger, Saver, gather_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement **`MAIN.PY`**\n",
    "####  _This implementation was originally intended to be run on the command-line, so we'll need to import the functions from **`main.py`** and explore._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate a command line **`Args`**\n",
    "Commandline arguments run the entire show, we'll have to simulate a command line entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_args = \"--num_episodes 500 --learn_rate 0.0001 --batch_size 64 -C 650\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = gather_args(cmd_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out what arguments have been loaded..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_args(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the **`Environment`**\n",
    "Now that args are loaded, we can load the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num Agents:\", env.agent_count)\n",
    "print(\"Action size:\", env.action_size)\n",
    "print(\"State size:\", env.state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take random actions in the Environment\n",
    "* Check that the environment is working\n",
    "* Test commands and see the results!\n",
    "\n",
    "While testing out the environment, set training mode to False, and limit max_steps to ensure it doesn't run too long for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.train = False\n",
    "env.reset()\n",
    "num_eps = 1\n",
    "state = env.state\n",
    "for _ in range(50):\n",
    "    score = np.zeros(env.agent_count)\n",
    "    env.reset()\n",
    "    actions = np.random.randint(env.agent_count, env.action_size)\n",
    "    next_state, reward, done = env.step(actions)\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_env_info(state, actions, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a trained Agent\n",
    "\n",
    "Let's see a trained agent in action! \n",
    "\n",
    "Preview in this notebook using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_eval_agent(args, env, \"weights/DDQN_v1_eps500_FINAL.agent\", num_eps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Or on the commandline, run the following command:**  \n",
    "**`python main.py -eval`**  \n",
    "*If you wish to see more than two episodes of evaluation, try this:*  \n",
    "**`python main.py --force_eval -num 5`**  \n",
    "\n",
    "You will be prompted to load a file on the command line, and as long as the provided weights are somewhere in tree below your current directory, the file should be found!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that the initial setup is created, training is as simple as running the **`train()`** routine. We'll take a look inside each step below.\n",
    "\n",
    "For Notebook purposes, saving has been disabled. Closing the environment is also disabled because the environment errors if closed and reopened in the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the **`Agent`** and **`Saver`** objects\n",
    "* The DQN_Agent object will use the framework specified in the commandline arguments\n",
    "* The Saver object will select a savename based on the framework, current time, and version-up if necessary. No files or folders are created until there is a file to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the params from args and the environment, set up an agent for training\n",
    "agent = DQN_Agent(env.state_size, env.action_size, args)\n",
    "\n",
    "# The Saver object will do all the saving and loading for the Agent\n",
    "saver = Saver(agent.framework, agent, args.save_dir, args.load_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, args, env, saver):\n",
    "    \"\"\"\n",
    "    Train the agent.\n",
    "    \"\"\"\n",
    "\n",
    "    logger = Logger(agent, args, saver.save_dir, log_every=50)\n",
    "\n",
    "    # Pre-fill the Replay Buffer\n",
    "    agent.initialize_memory(args.pretrain, env)\n",
    "\n",
    "    #Begin training loop\n",
    "    for episode in range(1, args.num_episodes+1):\n",
    "        # Begin each episode with a clean environment\n",
    "        done = False\n",
    "        env.reset()\n",
    "        # Get initial state\n",
    "        state = env.state\n",
    "        # Gather experience until done or max_steps is reached\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done:\n",
    "                next_state = None\n",
    "            agent.step(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "\n",
    "            logger.log(reward, agent)\n",
    "\n",
    "\n",
    "        #saver.save_checkpoint(agent, args.save_every)\n",
    "        agent.new_episode()\n",
    "        logger.step(episode, agent.epsilon)\n",
    "\n",
    "    #env.close()\n",
    "    #saver.save_final(agent)\n",
    "    #logger.graph(save_to_disk=False)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Reviewing each step*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create **`Logger`** object\n",
    "**`logger = Logger(agent, args, saver.save_dir, log_every=50, print_every=5)`**\n",
    "\n",
    "\n",
    "* Logger:\n",
    "    * prints status updates\n",
    "    * keeps track of rewards\n",
    "    * writes log files to disk\n",
    "    * creates a graph for review at the end of training\n",
    "    \n",
    "#### Initialize memory\n",
    "**`agent.initialize_memory(args.pretrain, env)`**\n",
    "\n",
    "* Learning cannot begin until the ReplayBuffer has at least as many memories as batch_size\n",
    "* In many cases, training is improved by collecting many random memories before learning from any given experience  \n",
    "    * _**`args.pretrain`**_ will fill the memory with however many random experience tuples as desired, usually in the thousands, although it was found during training that sometimes, setting this too high, reduces or eliminates convergence!\n",
    "    \n",
    "#### Training loop\n",
    "\n",
    "```python\n",
    "#Begin training loop\n",
    "for episode in range(1, args.num_episodes+1):\n",
    "    # Begin each episode with a clean environment\n",
    "    done = False\n",
    "    env.reset()\n",
    "    # Get initial state\n",
    "    state = env.state\n",
    "    # Gather experience until done or max_steps is reached\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        if done:\n",
    "            next_state = None\n",
    "        agent.step(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "\n",
    "        logger.log(reward, agent)\n",
    "```\n",
    "\n",
    "* Training will proceed for a specified number of episodes, in this code there is no implementation of early termination upon goal achievement.\n",
    "* Perform a standard Reinforcement Agent training loop:\n",
    "    * Get the initial state\n",
    "    * Select an action\n",
    "    * Collect next state, reward, and done-status from the environment after taking a step with the selected action\n",
    "    * Store the experience tuple of S, A, R, S'\n",
    "    * Rinse, wash, repeat\n",
    "* Inside of _**`agent.step`**_, the current experience is stored as a memory, and learning is then performed. This will be reviewed later in the AGENT review section.\n",
    "* Log the rewards for the current timestep\n",
    "\n",
    "#### Post Episode\n",
    "\n",
    "**`saver.save_checkpoint(agent, args.save_every)`**  \n",
    "At the end of each episode, the agent saves checkpoint weights every so often, as defined by the commandline arguments.\n",
    "\n",
    "**`agent.new_episode()`**  \n",
    "The agent then resets.\n",
    "\n",
    "**`logger.step(episode, agent.epsilon)`**  \n",
    "Logger keeps track of the scores and anneals the epsilon. *Annealing Epsilon through the Logger object is not ideal, and in later project code, this is more appropriately handled in the Agent itself*\n",
    "\n",
    "#### Post Training\n",
    "\n",
    "**`env.close()`**\n",
    "Close the environment.\n",
    "\n",
    "**`saver.save_final(agent)`**\n",
    "Save a final weights file.\n",
    "\n",
    "**`logger.graph()`**\n",
    "Create a graph from the Log data created during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What a training loop looks like in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To run a quick test, limit the length of training\n",
    "args.num_episodes = 15\n",
    "args.print_every = 3\n",
    "env.train = True\n",
    "args.eval = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(agent, args, env, saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "Before getting into the Agent, let's quickly review the Neural Network model, which is quite straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network Model. Nonlinear estimator for Qπ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, layer_sizes=[64, 64]):\n",
    "        \"\"\"\n",
    "        Initialize parameters and build model.\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(state_size, layer_sizes[0])])\n",
    "        self.output = nn.Linear(layer_sizes[-1], action_size)\n",
    "\n",
    "        layer_sizes = zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(i, o) for i, o in layer_sizes])\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Build a network that maps state -> action values.\n",
    "        \"\"\"\n",
    "\n",
    "        x = F.relu(self.hidden_layers[0](state))\n",
    "        for layer in self.hidden_layers[1:]:\n",
    "            x = F.relu(layer(x))\n",
    "        return self.output(x)\n",
    "```\n",
    "\n",
    "The network for estimating the Q-value takes in a state of **`state_size`** and outputs an estimate of size **`action_size`** which contains the estimated values of each action. For the Banana Collector environment this is a tensor of shape **\\[batchsize, 37]** and **\\[batchsize, 4]** respectively.\n",
    "\n",
    "The network can accomodate an abritrary number of hidden layers as specified by the user, however, for this environment there is no benefit to an overly large network and a default value of 64x64 is sufficient.\n",
    "\n",
    "The hidden layers use a ReLU activation function and simply output the final fully connected Linear Layer. Because we are outputting values and not probabilities, there is no activation function applied to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "### The Agent module is reviewed bit by bit.\n",
    "\n",
    "#### Class\n",
    "Initialize a Class to contain all of the Agent information and set up hyperparamaters.\n",
    "\n",
    "```python\n",
    "class DQN_Agent:\n",
    "    \"\"\"\n",
    "    PyTorch Implementation of DQN/DDQN.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, args):\n",
    "        \"\"\"\n",
    "        Initialize a D4PG Agent.\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        self.framework = args.framework\n",
    "        self.eval = args.eval\n",
    "        self.agent_count = 1\n",
    "        self.learn_rate = args.learn_rate\n",
    "        self.batch_size = args.batch_size\n",
    "        self.buffer_size = args.buffer_size\n",
    "        self.C = args.C\n",
    "        self._epsilon = args.epsilon\n",
    "        self.epsilon_decay = args.epsilon_decay\n",
    "        self.epsilon_min = args.epsilon_min\n",
    "        self.gamma = 0.99\n",
    "        self.rollout = args.rollout\n",
    "        self.tau = args.tau\n",
    "        self.momentum = 1\n",
    "        self.l2_decay = 0.0001\n",
    "        self.update_type = \"hard\"\n",
    "        self.t_step = 0\n",
    "        self.episode = 0\n",
    "        self.seed = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Networks\n",
    "Create the Experience Replay buffer of a chosen type, and initialize active and target networks. Copy the parameters from the active network to target network.\n",
    "\n",
    "\n",
    "```python        \n",
    "        # Set up memory buffers\n",
    "        if args.prioritized_experience_replay:\n",
    "            self.memory = PERBuffer(args.buffersize, \n",
    "                                    self.batchsize, \n",
    "                                    self.framestack, \n",
    "                                    self.device, \n",
    "                                    args.alpha, \n",
    "                                    args.beta)\n",
    "            self.criterion = WeightedLoss()\n",
    "        else:\n",
    "            self.memory = ReplayBuffer(self.device, \n",
    "                                       self.buffer_size, \n",
    "                                       self.gamma, \n",
    "                                       self.rollout)\n",
    "\n",
    "        #                    Initialize Q networks                         #\n",
    "        self.q = self._make_model(state_size, action_size, args.pixels)\n",
    "        self.q_target = self._make_model(state_size, action_size, args.pixels)\n",
    "        self._hard_update(self.q, self.q_target)\n",
    "        self.q_optimizer = self._set_optimizer(self.q.parameters(), \n",
    "                                               lr=self.learn_rate, \n",
    "                                               decay=self.l2_decay, \n",
    "                                               momentum=self.momentum)\n",
    "        self.new_episode()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`act()`**\n",
    "With probability of **`epsilon`**, the agent chooses a random action for encourage exploration. If not choosing a random exploratory action, the agent calculates the current Q-value for the provided state, and chooses the maximizing action for the next timestep.\n",
    "\n",
    "```python\n",
    "    def act(self, state, eval=False, pretrain=False):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy π.\n",
    "        Always use greedy if not training.\n",
    "        \"\"\"\n",
    "\n",
    "        if np.random.random() > self.epsilon or not eval and not pretrain:\n",
    "            state = state.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.q(state).detach().cpu()\n",
    "            action = action_values.argmax(dim=1).unsqueeze(0).numpy()\n",
    "        else:\n",
    "            action = np.random.randint(self.action_size, size=(1,1))\n",
    "        return action.astype(np.long)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`step()`**\n",
    "After collecting an experience tuple from the environment, the Agent stores the experience in the Replay Buffer. Memory storing happens at each timestep, and after enough experiences have been collected, then perform a **`learn()`** step.\n",
    "\n",
    "```python\n",
    "    def step(self, state, action, reward, next_state, pretrain=False):\n",
    "        \"\"\"\n",
    "        Add the current SARS' tuple into the short term memory, then learn\n",
    "        \"\"\"\n",
    "\n",
    "        # Current SARS' stored in short term memory, then stacked for NStep\n",
    "        experience = (state, action, reward, next_state)\n",
    "        if self.rollout == 1:\n",
    "            self.memory.store_trajectory(state, torch.from_numpy(action), torch.tensor([reward]), next_state)\n",
    "        else:\n",
    "            self.memory.store_experience(experience)\n",
    "        self.t_step += 1\n",
    "\n",
    "        # Learn after done pretraining\n",
    "        if not pretrain:\n",
    "            self.learn()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`learn()`**\n",
    "This is where the magic happens, the commented code fairly explicitly outlines each step, but we can briefly overview here as well.\n",
    "\n",
    "* First, select a sample of experiences from the Replay Buffer. If using a Prioritized Experience Replay implementation, then this sample will be motivated by estimations of how much there is to be learned from each memory, but in standard ReplayBuffer implementation, the sample is fully random from the buffer.\n",
    "* Then calculate the Q-values for the current state, and select the maximum value from the returns. If using DDQN training, then select the value which corresponds to the maximizing action under the **active** as opposed to the target network.\n",
    "* Perform a Bellman value estimation step by adding the current, *real rewards*, to the estimated value of the next state. If using n-step bootstrapping, then the value of the next state will be N+1 steps ahead of the current state, and the *real rewards* will be the sum of the next N timesteps.\n",
    "* Calculate the loss by taking the gradient of the distance between the predicted values and the current values. Huber Loss is the current state-of-the-art for achieving consistent results in backpropogation. \n",
    "* If using PER, then Huber Loss must be calculated with a custom function that can take into account importance sampling weights. This is not fully tested, although implemented, in the current codebase.\n",
    "            \n",
    "            \n",
    "``` python            \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Trains the Deep QNetwork and returns action values.\n",
    "        Can use multiple frameworks.\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample from replay buffer, REWARDS are sum of (ROLLOUT - 1) timesteps\n",
    "        # Already calculated before storing in the replay buffer.\n",
    "        # NEXT_STATES are ROLLOUT steps ahead of STATES\n",
    "        batch, is_weights, tree_idx = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, terminal_mask = batch\n",
    "\n",
    "        q_values = torch.zeros(self.batch_size).to(self.device)\n",
    "        if self.framework == 'DQN':\n",
    "            # Max predicted Q values for the next states from the target model\n",
    "            q_values[terminal_mask] = self.q_target(next_states).detach().max(dim=1)[0]\n",
    "\n",
    "        if self.framework == 'DDQN':\n",
    "            # Get maximizing ACTION under Q, evaluate actionvalue\n",
    "            # under q_target\n",
    "\n",
    "            # Max valued action under active network\n",
    "            max_actions = self.q(next_states).detach().argmax(1).unsqueeze(1)\n",
    "            # Use the active network action to get the value of the stable\n",
    "            # target network\n",
    "            q_values[terminal_mask] = self.q_target(next_states).detach().gather(1, max_actions).squeeze(1)\n",
    "\n",
    "        targets = rewards + (self.gamma**self.rollout * q_values)\n",
    "\n",
    "        targets = targets.unsqueeze(1)\n",
    "        values = self.q(states).gather(1, actions)\n",
    "\n",
    "        #Huber Loss provides better results than MSE\n",
    "        if is_weights is None:\n",
    "            loss = F.smooth_l1_loss(values, targets)\n",
    "\n",
    "        #Compute Huber Loss manually to utilize is_weights with Prioritization\n",
    "        else:\n",
    "            loss, td_errors = self.criterion.huber(values, targets, is_weights)\n",
    "            self.memory.batch_update(tree_idx, td_errors)\n",
    "\n",
    "        # Perform gradient descent\n",
    "        self.q_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "\n",
    "        self._update_networks()\n",
    "        self.loss = loss.item()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`initialize_memory()`**\n",
    "Perform a basic loop through the environment for **`pretrain`** number of steps by collecting experiences with random actions. \n",
    "\n",
    "```python\n",
    "    def initialize_memory(self, pretrain_length, env):\n",
    "        \"\"\"\n",
    "        Fills up the ReplayBuffer memory with PRETRAIN_LENGTH number of experiences\n",
    "        before training begins.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.memory) >= pretrain_length:\n",
    "            print(\"Memory already filled, length: {}\".format(len(self.memory)))\n",
    "            return\n",
    "\n",
    "        print(\"Initializing memory buffer.\")\n",
    "\n",
    "        while True:\n",
    "            done = False\n",
    "            env.reset()\n",
    "            state = env.state\n",
    "            while not done:\n",
    "                action = self.act(state, pretrain=True)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                if done:\n",
    "                    next_state = None\n",
    "\n",
    "                self.step(state, action, reward, next_state, pretrain=True)\n",
    "                states = next_state\n",
    "\n",
    "                if self.t_step % 50 == 0 or len(self.memory) >= pretrain_length:\n",
    "                    print(\"Taking pretrain step {}... memory filled: {}/{}\\\n",
    "                        \".format(self.t_step, len(self.memory), pretrain_length))\n",
    "                if len(self.memory) >= pretrain_length:\n",
    "                    print(\"Done!\")\n",
    "                    self.t_step = 0\n",
    "                    self._epsilon = 1\n",
    "                    return\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`epsilon`**\n",
    "Epsilon controls the probability of exploratory action, and must be annealed to some low value <<1 as training progresses. The below code is a very basic annealing curve which should, in a more robust implementation, have some correlation to training performance. **`epsilon`** is here implemented as a class Property such that every time it is called, the value changes. This is primarily for neatness.\n",
    "\n",
    "```python\n",
    "    @property\n",
    "    def epsilon(self):\n",
    "        \"\"\"\n",
    "        This property ensures that the annealing process is run every time that\n",
    "        E is called.\n",
    "        Anneals the epsilon rate down to a specified minimum to ensure there is\n",
    "        always some noisiness to the policy actions. Returns as a property.\n",
    "        \"\"\"\n",
    "\n",
    "        self._epsilon = max(self.epsilon_min, self.epsilon_decay ** self.t_step)\n",
    "        return self._epsilon   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate\n",
    "The below methods perform basic tasks that are explained adequately in their commenting.\n",
    "\n",
    "```python                \n",
    "    def new_episode(self):\n",
    "        \"\"\"\n",
    "        Handle any cleanup or steps to begin a new episode of training.\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory.init_n_step()\n",
    "        self.episode += 1\n",
    "\n",
    "    def _update_networks(self):\n",
    "        \"\"\"\n",
    "        Updates the network using either DDPG-style soft updates (w/ param \\\n",
    "        TAU), or using a DQN/D4PG style hard update every C timesteps.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.update_type == \"soft\":\n",
    "            self._soft_update(self.q, self.q_target)\n",
    "        elif self.t_step % self.C == 0:\n",
    "            self._hard_update(self.q, self.q_target)\n",
    "\n",
    "    def _soft_update(self, active, target):\n",
    "        \"\"\"\n",
    "        Slowly updated the network using every-step partial network copies\n",
    "        modulated by parameter TAU.\n",
    "        \"\"\"\n",
    "\n",
    "        for t_param, param in zip(target.parameters(), active.parameters()):\n",
    "            t_param.data.copy_(self.tau*param.data + (1-self.tau)*t_param.data)\n",
    "\n",
    "    def _hard_update(self, active, target):\n",
    "        \"\"\"\n",
    "        Fully copy parameters from active network to target network. To be used\n",
    "        in conjunction with a parameter \"C\" that modulated how many timesteps\n",
    "        between these hard updates.\n",
    "        \"\"\"\n",
    "\n",
    "        target.load_state_dict(active.state_dict())\n",
    "\n",
    "    def _set_optimizer(self, params,lr, decay, momentum,  optimizer=\"Adam\"):\n",
    "        \"\"\"\n",
    "        Sets the optimizer based on command line choice. Defaults to Adam.\n",
    "        \"\"\"\n",
    "\n",
    "        if optimizer == \"RMSprop\":\n",
    "            return optim.RMSprop(params, lr=lr, momentum=momentum)\n",
    "        elif optimizer == \"SGD\":\n",
    "            return optim.SGD(params, lr=lr, momentum=momentum)\n",
    "        else:\n",
    "            return optim.Adam(params, lr=lr, weight_decay=decay)\n",
    "\n",
    "    def _make_model(self, state_size, action_size, use_cnn):\n",
    "        \"\"\"\n",
    "        Sets up the network model based on whether state data or pixel data is\n",
    "        provided.\n",
    "        \"\"\"\n",
    "\n",
    "        if use_cnn:\n",
    "            return QCNNetwork(state_size, action_size, self.seed).to(self.device)\n",
    "        else:\n",
    "            return QNetwork(state_size, action_size, self.seed).to(self.device)     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping it up\n",
    "\n",
    "This was a fun project as an introduction to Reinforcement Learning. I will look forward to more challenging or complex environments to put this knowledge and methodology into more robust use. \n",
    "\n",
    "I also learned a lot about implementation and flexibility during this project, rewriting the entire code structure several times. Eventually it would be good to update this code into an even more mature structure, as evidenced in Project \\#3, *Collaborate and Compete*\n",
    "\n",
    "To run this project as it was intended, please review the README.md in the github repository located at:\n",
    "https://github.com/whiterabbitobj/Udacity-DeepRL/tree/master/Projects/Navigation\n",
    "\n",
    "The agent params have already been optimized in their defaults and this project can be run as simply as:  \n",
    "**`python main.py`**   \n",
    "to achieve good, clean results.\n",
    "\n",
    "Thanks for reading!\n",
    "\n",
    "Please feel free to contact me with any bugs, questions, or comments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
