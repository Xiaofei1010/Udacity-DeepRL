{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><br><br><b><font size=10> NAVIGATION </font></b><br><br>\n",
    "<font size = 4><i>...implementation for Udacity Deep Reinforcement Learning \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Imports for the notebook\n",
    "This Notebook uses code from separate python files where most of the implementation is handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from agent import DQN_Agent\n",
    "from environment import Environment\n",
    "from data_handling import Logger, Saver, gather_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement MENU.PY\n",
    "#### <i> This implementation was originally intended to be run on the command-line, so let's import the functions from main.py and explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commandline arguments run the entire show, so we'll need to manually declare them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_args = \"--num_episodes 500 --learn_rate 0.0001 --batch_size 64 -C 650\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = gather_args(cmd_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out what arguments have been loaded..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join([\"{}: {}\".format(arg, getattr(args, arg)) for arg in vars(args)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the world\n",
    "Now that args are loaded, set up the remainder of the groundwork to prepare for running the Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment using the above ar\n",
    "env = Environment(args)\n",
    "\n",
    "# Using the params from args and the environment, set up an agent for training\n",
    "agent = DQN_Agent(env.state_size,\n",
    "                  env.action_size,\n",
    "                  args)\n",
    "\n",
    "# The Saver object will do all the saving and loading for the Agent\n",
    "saver = Saver(agent.framework, agent, args.save_dir, args.load_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"State size:\", env.state_size)\n",
    "print(\"Action size:\", env.action_size)\n",
    "print(\"Num Agents:\", env.agent_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take random actions in the Environment\n",
    "* Check that the environment is working\n",
    "* Test commands and see the results!\n",
    "\n",
    "While testing out the environment, set training mode to False, and limit max_steps to ensure it doesn't run too long for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.train = False\n",
    "env.reset()\n",
    "max_steps = 100\n",
    "scores = np.zeros(env.agent_count)\n",
    "arewards = np.empty((max_steps, env.agent_count))\n",
    "states = env.states\n",
    "actions = np.zeros((20,4))\n",
    "for i in range(max_steps):\n",
    "    actions = np.random.randn(env.agent_count, env.action_size)\n",
    "    #actions[:,3] += .01\n",
    "    actions = np.clip(actions, -1, 1)\n",
    "    next_states, rewards, dones = env.step(actions)\n",
    "    scores += rewards\n",
    "    arewards[i] = rewards\n",
    "    states = next_states\n",
    "#     if np.any(dones):\n",
    "#         break\n",
    "#    i += 1\n",
    "#print(arewards)\n",
    "print(arewards.min())\n",
    "print(arewards.max())\n",
    "#print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Force-reload modules as they're updated\n",
    "<i> This notebook was developed as the code is written in Atom, the below cell reloads the modules as they're needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import agent\n",
    "import buffers\n",
    "importlib.reload(models)\n",
    "importlib.reload(agent)\n",
    "importlib.reload(buffers)\n",
    "from agent import D4PG_Agent\n",
    "env.train = True\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the AGENT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4pg_agent = D4PG_Agent(env.state_size, env.action_size, env.agent_count, args.alr, args.clr, args.batch, args.buffer, args.C)\n",
    "\n",
    "d4pg_agent.initialize_memory(args.pretrain, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d4pg_agent.critic.fc1.weight.data)\n",
    "print(d4pg_agent.critic_target.fc1.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out Actor actions without training\n",
    "<i> Test the <b>Actor</b> network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "scores = np.zeros(env.agent_count)\n",
    "states = env.states\n",
    "for i in range(30):\n",
    "    actions = d4pg_agent.act(states)\n",
    "    \n",
    "    # Print sample actions returned by the ACTOR network\n",
    "    print(\"ACTIONS:\", actions[1])\n",
    "    \n",
    "    next_states, rewards, dones = env.step(actions)\n",
    "    scores += rewards\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "    i += 1\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out Critic scores without training\n",
    "<i> Test the <b>Critic</b> network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "scores = np.zeros(env.agent_count)\n",
    "states = env.states\n",
    "for i in range(2):\n",
    "    actions = d4pg_agent.act(states) \n",
    "    ns, rewards, dones = env.step(actions)\n",
    "    scores += rewards\n",
    "    \n",
    "    # Print sample distributions returned by the CRITIC\n",
    "    batch = d4pg_agent.memory.sample(batch_size=1)\n",
    "    states = torch.cat(batch.state).to(DEVICE)\n",
    "    actions = torch.cat(batch.action).float().to(DEVICE)\n",
    "    rewards = torch.cat(batch.reward).to(DEVICE)\n",
    "    next_states = torch.cat(batch.next_state).to(DEVICE) \n",
    "\n",
    "    dist, probs = d4pg_agent.critic_target(next_states, d4pg_agent.actor(next_states))\n",
    "    proj_dist = d4pg_agent._get_targets(rewards, next_states)\n",
    "    ldist, lprobs = d4pg_agent.critic(next_states, d4pg_agent.actor(next_states))\n",
    "    \n",
    "    print(\"DIST: \", dist)\n",
    "    print(\"Local DIST: \", ldist)\n",
    "#     print(\"PROBS: \", probs)\n",
    "#     print(\"PROJECTED DIST: \", proj_dist)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    states = ns\n",
    "    if np.any(dones):\n",
    "        break\n",
    "    i += 1\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Test out various Agent functionality\n",
    "\n",
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.alr = 1e-4\n",
    "args.clr = 1e-4\n",
    "args.batch = 128\n",
    "args.buffer = 100000\n",
    "args.C = 4000\n",
    "vmin = 0\n",
    "vmax = 0.1\n",
    "num_atoms = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import agent\n",
    "import buffers\n",
    "importlib.reload(models)\n",
    "importlib.reload(agent)\n",
    "importlib.reload(buffers)\n",
    "from agent import D4PG_Agent\n",
    "d4pg_agent = D4PG_Agent(env.state_size, env.action_size, env.agent_count, args.alr, args.clr, args.batch, args.buffer, args.C)\n",
    "env.train = True\n",
    "env.reset()\n",
    "#d4pg_agent.initialize_memory(args.pretrain, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.train = False\n",
    "env.reset()\n",
    "file = 'saves/D4PG_20190306_v17_eps100_FINAL.agent'\n",
    "checkpoint = torch.load(file, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4pg_agent.actor.load_state_dict(checkpoint['actor_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.max_time = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= torch.linspace(vmin, vmax, num_atoms)\n",
    "print(t)\n",
    "print(t.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#w_old = torch.zeros(d4pg_agent.actor.output.weight.data[0].shape)\n",
    "\n",
    "for episode in range(1, 2):\n",
    "    # Begin each episode with a clean environment\n",
    "    env.reset()\n",
    "    # Get initial state\n",
    "    states = env.states\n",
    "    scores = np.zeros(env.agent_count)\n",
    "    # Gather experience for a maximum amount of steps, or until Done,\n",
    "    # whichever comes first\n",
    "    for t in range(args.max_time):\n",
    "        actions = d4pg_agent.act(states)\n",
    "        next_states, rewards, dones = env.step(actions)\n",
    "        d4pg_agent.step(states, actions, rewards, next_states)\n",
    "        states = next_states\n",
    "\n",
    "        scores += rewards\n",
    "        if np.any(dones):\n",
    "            break\n",
    "        print(\"A LOSS: \", d4pg_agent.actor_loss)\n",
    "        print(\"C LOSS: \", d4pg_agent.critic_loss)\n",
    "    print(\"Episode rewards: \", scores.mean())\n",
    "    agent.new_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn about how the Categorical Bellman step works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.random.randn(env.agent_count, env.action_size)\n",
    "actions = np.clip(actions, -1, 1).astype(np.float32)\n",
    "states = torch.from_numpy(env.states).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, log_probs = critic(states, torch.from_numpy(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs.shape)\n",
    "print(log_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Container():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "c = Container()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = -10\n",
    "vmax = 10\n",
    "natoms = 51\n",
    "gamma = .99\n",
    "atoms = torch.linspace(vmin, vmax, natoms)\n",
    "delta_z = (vmax - vmin) / (natoms -1)\n",
    "r = torch.tensor(rewards).unsqueeze(-1)\n",
    "\n",
    "probs = probs.detach()\n",
    "q_next = (probs * atoms).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### projected atoms\n",
    "<html><i>\n",
    "<b>atoms.view(1,-1)</b> becomes shape [1, num_atoms]\n",
    "<br>\n",
    "<b>r</b> is unsqueezed in the last (-1) dimension, so it's shape [20,1]\n",
    "<br>\n",
    "the result is a tensor that holds an offset (projected) version of the atoms for each reward instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tz = projected atoms, atoms (values) projected by scaling and offsetting via the bellman equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = r + gamma * atoms.view(1,-1)\n",
    "tz.clamp_(vmin, vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### computes \"bj\" from the pseudo-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (tz - vmin) / c.delta_z\n",
    "b[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### l/u in the psuedocode are LOWER and UPPER bounds on the supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = b.floor().long()\n",
    "u = b.ceil().long()\n",
    "print(l[0])\n",
    "print(u[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### m_l/m_u are computed in the pseudocode under \"distribute the probability of tz\", but still a bit opaque to me on how it should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = (u.float() + (l == u).float() - b) * c.probs\n",
    "ml[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = (b - l.float()) * c.probs\n",
    "mu[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prob = torch.tensor(np.zeros(probs.size()))\n",
    "for i in range(target_prob.size(0)):\n",
    "    target_prob[i].index_add_(0, l[i].long(), ml[i].double())\n",
    "    target_prob[i].index_add_(0, u[i].long(), mu[i].double())\n",
    "target_prob[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the environment when finished with the code/training/etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
