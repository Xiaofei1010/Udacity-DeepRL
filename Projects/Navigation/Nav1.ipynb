{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><br><br><b><font size=10> NAVIGATION </font></b><br><br>\n",
    "<font size = 4><i>...implementation for Udacity Deep Reinforcement Learning \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before getting into the nitty gritty...\n",
    "#### Here are some results of the Agent detailed below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m_repr_mimebundle_\u001b[1;34m(self, include, exclude)\u001b[0m\n\u001b[0;32m   1242\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[0mmimetype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mimetype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1244\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malways_both\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1245\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m                 \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mmimetype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[1;34m(self, always_both)\u001b[0m\n\u001b[0;32m   1251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_data_and_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malways_both\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m         \u001b[1;34m\"\"\"shortcut for returning metadata with shape information, if defined\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1253\u001b[1;33m         \u001b[0mb64_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1254\u001b[0m         \u001b[0mmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1268\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_repr_png_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FMT_PNG\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1270\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_repr_jpeg_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[1;34m(self, always_both)\u001b[0m\n\u001b[0;32m   1251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_data_and_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malways_both\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m         \u001b[1;34m\"\"\"shortcut for returning metadata with shape information, if defined\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1253\u001b[1;33m         \u001b[0mb64_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1254\u001b[0m         \u001b[0mmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Imports\n",
    "This Notebook uses code from separate python files where most of the implementation is handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from agent import DQN_Agent\n",
    "from environment import Environment\n",
    "from data_handling import Logger, Saver, gather_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement MENU.PY\n",
    "#### <i> This implementation was originally intended to be run on the command-line, so we'll need to import the functions from main.py and explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate a command line\n",
    "Commandline arguments run the entire show, we'll have to simulate a command line entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_args = \"--num_episodes 500 --learn_rate 0.0001 --batch_size 64 -C 650\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = gather_args(cmd_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out what arguments have been loaded..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_args(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Environment\n",
    "Now that args are loaded, we can load the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num Agents:\", env.agent_count)\n",
    "print(\"Action size:\", env.action_size)\n",
    "print(\"State size:\", env.state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take random actions in the Environment\n",
    "* Check that the environment is working\n",
    "* Test commands and see the results!\n",
    "\n",
    "While testing out the environment, set training mode to False, and limit max_steps to ensure it doesn't run too long for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.train = False\n",
    "env.reset()\n",
    "num_eps = 1\n",
    "state = env.state\n",
    "for _ in range(50):\n",
    "    score = np.zeros(env.agent_count)\n",
    "    env.reset()\n",
    "    actions = np.random.randint(env.agent_count, env.action_size)\n",
    "    next_state, reward, done = env.step(actions)\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the `Agent` and `Saver` objects\n",
    "* The DQN_Agent object will use the framework specified in the commandline arguments\n",
    "* The Saver object will select a savename based on the framework, current time, and version-up if necessary. No files or folders are created until there is a file to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the params from args and the environment, set up an agent for training\n",
    "agent = DQN_Agent(env.state_size, env.action_size, args)\n",
    "\n",
    "# The Saver object will do all the saving and loading for the Agent\n",
    "saver = Saver(agent.framework, agent, args.save_dir, args.load_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agent\n",
    "Now that the initial setup is created, training is as simple as running the `train()` routine. We'll take a look inside each step below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, args, env, saver):\n",
    "    \"\"\"\n",
    "    Train the agent.\n",
    "    \"\"\"\n",
    "\n",
    "    logger = Logger(agent, args, saver.save_dir, log_every=50, print_every=5)\n",
    "\n",
    "    # Pre-fill the Replay Buffer\n",
    "    agent.initialize_memory(args.pretrain, env)\n",
    "\n",
    "    #Begin training loop\n",
    "    for episode in range(1, args.num_episodes+1):\n",
    "        # Begin each episode with a clean environment\n",
    "        done = False\n",
    "        env.reset()\n",
    "        # Get initial state\n",
    "        state = env.state\n",
    "        # Gather experience until done or max_steps is reached\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done:\n",
    "                next_state = None\n",
    "            agent.step(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "\n",
    "            logger.log(reward, agent)\n",
    "\n",
    "\n",
    "        saver.save_checkpoint(agent, args.save_every)\n",
    "        agent.new_episode()\n",
    "        logger.step(episode, agent.epsilon)\n",
    "\n",
    "    env.close()\n",
    "    saver.save_final(agent)\n",
    "    logger.graph()\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Reviewing each step*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create `Logger` object\n",
    "`logger = Logger(agent, args, saver.save_dir, log_every=50, print_every=5)`\n",
    "\n",
    "\n",
    "* Logger:\n",
    "    * prints status updates\n",
    "    * keeps track of rewards\n",
    "    * writes log files to disk\n",
    "    * creates a graph for review at the end of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize memory\n",
    "`agent.initialize_memory(args.pretrain, env)`\n",
    "\n",
    "* Learning cannot begin until the ReplayBuffer has at least as many memories as batch_size\n",
    "* In many cases, training is improved by collecting many random memories before learning from any given experience  \n",
    "    * *`args.pretrain`* will fill the memory with however many random experience tuples as desired, usually in the thousands, although it was found during training that sometimes, setting this too high, reduces or eliminates convergence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the training loop\n",
    "```python\n",
    "#Begin training loop\n",
    "for episode in range(1, args.num_episodes+1):\n",
    "    # Begin each episode with a clean environment\n",
    "    done = False\n",
    "    env.reset()\n",
    "    # Get initial state\n",
    "    state = env.state\n",
    "    # Gather experience until done or max_steps is reached\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        if done:\n",
    "            next_state = None\n",
    "        agent.step(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "\n",
    "        logger.log(reward, agent)\n",
    "```\n",
    "\n",
    "* Training will proceed for a specified number of episodes, in this code there is no implementation of early termination upon goal achievement.\n",
    "* Perform a standard Reinforcement Agent training loop:\n",
    "    * Get the initial state\n",
    "    * Select an action\n",
    "    * Collect next state, reward, and done-status from the environment after taking a step with the selected action\n",
    "    * Store the experience tuple of S, A, R, S'\n",
    "    * Rinse, wash, repeat\n",
    "* Inside of *`agent.step`*, the current experience is stored as a memory, and learning is then performed. This will be reviewed later in the AGENT review section.\n",
    "* Log the rewards for the current timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To run a quick test, limit the length of training\n",
    "args.num_episodes = 5\n",
    "args.print_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(agent, args, env, saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out Actor actions without training\n",
    "<i> Test the <b>Actor</b> network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "scores = np.zeros(env.agent_count)\n",
    "states = env.states\n",
    "for i in range(30):\n",
    "    actions = d4pg_agent.act(states)\n",
    "    \n",
    "    # Print sample actions returned by the ACTOR network\n",
    "    print(\"ACTIONS:\", actions[1])\n",
    "    \n",
    "    next_states, rewards, dones = env.step(actions)\n",
    "    scores += rewards\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "    i += 1\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out Critic scores without training\n",
    "<i> Test the <b>Critic</b> network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "scores = np.zeros(env.agent_count)\n",
    "states = env.states\n",
    "for i in range(2):\n",
    "    actions = d4pg_agent.act(states) \n",
    "    ns, rewards, dones = env.step(actions)\n",
    "    scores += rewards\n",
    "    \n",
    "    # Print sample distributions returned by the CRITIC\n",
    "    batch = d4pg_agent.memory.sample(batch_size=1)\n",
    "    states = torch.cat(batch.state).to(DEVICE)\n",
    "    actions = torch.cat(batch.action).float().to(DEVICE)\n",
    "    rewards = torch.cat(batch.reward).to(DEVICE)\n",
    "    next_states = torch.cat(batch.next_state).to(DEVICE) \n",
    "\n",
    "    dist, probs = d4pg_agent.critic_target(next_states, d4pg_agent.actor(next_states))\n",
    "    proj_dist = d4pg_agent._get_targets(rewards, next_states)\n",
    "    ldist, lprobs = d4pg_agent.critic(next_states, d4pg_agent.actor(next_states))\n",
    "    \n",
    "    print(\"DIST: \", dist)\n",
    "    print(\"Local DIST: \", ldist)\n",
    "#     print(\"PROBS: \", probs)\n",
    "#     print(\"PROJECTED DIST: \", proj_dist)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    states = ns\n",
    "    if np.any(dones):\n",
    "        break\n",
    "    i += 1\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Test out various Agent functionality\n",
    "\n",
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.alr = 1e-4\n",
    "args.clr = 1e-4\n",
    "args.batch = 128\n",
    "args.buffer = 100000\n",
    "args.C = 4000\n",
    "vmin = 0\n",
    "vmax = 0.1\n",
    "num_atoms = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import agent\n",
    "import buffers\n",
    "importlib.reload(models)\n",
    "importlib.reload(agent)\n",
    "importlib.reload(buffers)\n",
    "from agent import D4PG_Agent\n",
    "d4pg_agent = D4PG_Agent(env.state_size, env.action_size, env.agent_count, args.alr, args.clr, args.batch, args.buffer, args.C)\n",
    "env.train = True\n",
    "env.reset()\n",
    "#d4pg_agent.initialize_memory(args.pretrain, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.train = False\n",
    "env.reset()\n",
    "file = 'saves/D4PG_20190306_v17_eps100_FINAL.agent'\n",
    "checkpoint = torch.load(file, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4pg_agent.actor.load_state_dict(checkpoint['actor_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.max_time = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= torch.linspace(vmin, vmax, num_atoms)\n",
    "print(t)\n",
    "print(t.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#w_old = torch.zeros(d4pg_agent.actor.output.weight.data[0].shape)\n",
    "\n",
    "for episode in range(1, 2):\n",
    "    # Begin each episode with a clean environment\n",
    "    env.reset()\n",
    "    # Get initial state\n",
    "    states = env.states\n",
    "    scores = np.zeros(env.agent_count)\n",
    "    # Gather experience for a maximum amount of steps, or until Done,\n",
    "    # whichever comes first\n",
    "    for t in range(args.max_time):\n",
    "        actions = d4pg_agent.act(states)\n",
    "        next_states, rewards, dones = env.step(actions)\n",
    "        d4pg_agent.step(states, actions, rewards, next_states)\n",
    "        states = next_states\n",
    "\n",
    "        scores += rewards\n",
    "        if np.any(dones):\n",
    "            break\n",
    "        print(\"A LOSS: \", d4pg_agent.actor_loss)\n",
    "        print(\"C LOSS: \", d4pg_agent.critic_loss)\n",
    "    print(\"Episode rewards: \", scores.mean())\n",
    "    agent.new_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn about how the Categorical Bellman step works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.random.randn(env.agent_count, env.action_size)\n",
    "actions = np.clip(actions, -1, 1).astype(np.float32)\n",
    "states = torch.from_numpy(env.states).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, log_probs = critic(states, torch.from_numpy(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs.shape)\n",
    "print(log_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Container():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "c = Container()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = -10\n",
    "vmax = 10\n",
    "natoms = 51\n",
    "gamma = .99\n",
    "atoms = torch.linspace(vmin, vmax, natoms)\n",
    "delta_z = (vmax - vmin) / (natoms -1)\n",
    "r = torch.tensor(rewards).unsqueeze(-1)\n",
    "\n",
    "probs = probs.detach()\n",
    "q_next = (probs * atoms).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### projected atoms\n",
    "<html><i>\n",
    "<b>atoms.view(1,-1)</b> becomes shape [1, num_atoms]\n",
    "<br>\n",
    "<b>r</b> is unsqueezed in the last (-1) dimension, so it's shape [20,1]\n",
    "<br>\n",
    "the result is a tensor that holds an offset (projected) version of the atoms for each reward instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tz = projected atoms, atoms (values) projected by scaling and offsetting via the bellman equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = r + gamma * atoms.view(1,-1)\n",
    "tz.clamp_(vmin, vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### computes \"bj\" from the pseudo-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (tz - vmin) / c.delta_z\n",
    "b[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### l/u in the psuedocode are LOWER and UPPER bounds on the supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = b.floor().long()\n",
    "u = b.ceil().long()\n",
    "print(l[0])\n",
    "print(u[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### m_l/m_u are computed in the pseudocode under \"distribute the probability of tz\", but still a bit opaque to me on how it should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = (u.float() + (l == u).float() - b) * c.probs\n",
    "ml[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = (b - l.float()) * c.probs\n",
    "mu[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prob = torch.tensor(np.zeros(probs.size()))\n",
    "for i in range(target_prob.size(0)):\n",
    "    target_prob[i].index_add_(0, l[i].long(), ml[i].double())\n",
    "    target_prob[i].index_add_(0, u[i].long(), mu[i].double())\n",
    "target_prob[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the environment when finished with the code/training/etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
